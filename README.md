# StrongSORT with OSNet for YOLOv7
This is a forked modification of [StrongSORT-YOLO](https://github.com/bharath5673/StrongSORT-YOLO).

<div align="center">
<p>
<img src="MOT16_eval/track_pedestrians.gif" width="300"/>  <img src="MOT16_eval/track_all.gif" width="300"/> 
</p>
</div>


## Introduction
This repository contains a highly configurable two-stage-tracker that adjusts to different deployment scenarios. The detections generated by [YOLOv7](https://github.com/WongKinYiu/yolov7) are passed to [StrongSORT](https://github.com/dyhBUPT/StrongSORT) which combines motion and appearance information based on [OSNet](https://github.com/KaiyangZhou/deep-person-reid) in order to tracks the objects. It can track any object that your YOLOv7 model was trained to detect. The algorith uses a forked version of [YOLOv7](https://github.com/nelioasousa/yolov7), since the original is no longer maintained.


## Before you run the tracker
1. Clone the repository recursively:

`git clone --recurse-submodules https://github.com/nelioasousa/strongsort-yolo.git`

If you already cloned and forgot to use `--recurse-submodules` you can run `git submodule update --init`

2. Make sure that you fulfill all the requirements: Python 3.8 or later with all required dependencies installed, including torch>=1.7. To install, run:

`pip install -r requirements.txt`


## Tracking sources
Tracking can be runned on videos compatible with OpenCV's VideoCapture class. Also supports sequence of images, as long as those can be opened with cv2.imread() method.


## Select YOLOv7 object detector checkpoint
There is a clear trade-off between model inference speed and accuracy.

```bash
$ python track.py ... --yolo-weights weights/yolov7-tiny.pt --img 640
                                              yolov7.pt
                                              yolov7x.pt 
                                              yolov7-w6.pt 
                                              yolov7-e6.pt 
                                              yolov7-d6.pt 
                                              yolov7-e6e.pt
                                              ...
```

## Filter tracked classes
By default the tracker tracks all classes. If you want to track a subset of the classes, add their corresponding index after the `--classes` flag.

```bash
python track.py ... --classes 16 17  # tracks classes with ids 16 e 17
```


### Draw Objects Trajectories
To draw the trajectory lines of the objects in the output video, call the flags `--save-vid` AND `--draw-trajectory`. Without `--draw-trajectory` the output video will contain only tracking bouding boxes. Customize the bboxes labels with `--hide-labels`, `--hide-conf` and `--hide-class`.

```bash
$ python track.py --source test.mp4 --yolo-weights weights/*.pt --save-vid --draw-trajectory
```

[Here](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) is a list of all the possible objects that a YOLOv7 model trained on MS COCO can detect. Notice that the indexing for the classes in this repo starts at zero.


## MOT compliant results
Can be saved to `project/name/labels/example.txt` by calling `--save-txt` AND `--mot-format` flags.

```bash
python track.py --source example.mp4 --project ? --name ? --save-txt --mot-format ...
```


## Thanks to
This project was only possible thanks to the efforts of the following:

```latex
@article{wang2022yolov7,
  title={{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author={Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  journal={arXiv preprint arXiv:2207.02696},
  year={2022}
}

@article{du2023strongsort,
  title={Strongsort: Make deepsort great again},
  author={Du, Yunhao and Zhao, Zhicheng and Song, Yang and Zhao, Yanyun and Su, Fei and Gong, Tao and Meng, Hongying},
  journal={IEEE Transactions on Multimedia},
  year={2023},
  publisher={IEEE}
}

@article{torchreid,
  title={Torchreid: A Library for Deep Learning Person Re-Identification in Pytorch},
  author={Zhou, Kaiyang and Xiang, Tao},
  journal={arXiv preprint arXiv:1910.10093},
  year={2019}
}
```

<details><summary> <b>Others</b> </summary>

* [https://github.com/bharath5673/StrongSORT-YOLO](https://github.com/bharath5673/StrongSORT-YOLO)
* [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)
* [https://github.com/WongKinYiu/yolor](https://github.com/WongKinYiu/yolor)
* [https://github.com/WongKinYiu/PyTorch_YOLOv4](https://github.com/WongKinYiu/PyTorch_YOLOv4)
* [https://github.com/WongKinYiu/ScaledYOLOv4](https://github.com/WongKinYiu/ScaledYOLOv4)
* [https://github.com/Megvii-BaseDetection/YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)
* [https://github.com/ultralytics/yolov3](https://github.com/ultralytics/yolov3)
* [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)
* [https://github.com/DingXiaoH/RepVGG](https://github.com/DingXiaoH/RepVGG)
* [https://github.com/JUGGHM/OREPA_CVPR2022](https://github.com/JUGGHM/OREPA_CVPR2022)
* [https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose](https://github.com/TexasInstruments/edgeai-yolov5/tree/yolo-pose)
</details>
